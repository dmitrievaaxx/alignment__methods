name: grpo_training
desc: Starting GRPO training

hydra:
  searchpath:
    - /job/verl/verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  train_files: /job/train.parquet
  val_files: /job/val.parquet
  max_prompt_length: 1024
  max_response_length: 3072
  train_batch_size: 256
  filter_overlong_prompts: True
  apply_chat_template_kwargs: {}
  filter_overlong_prompts_workers: 12
  prompt_key: prompt
  reward_fn_key: data_source
  return_raw_chat: True

actor_rollout_ref:
 model:
   path: "Qwen/Qwen2.5-1.5B-Instruct"
   enable_gradient_checkpointing: true
   enable_activation_offload: true # true
   use_remove_padding: false # true
   use_fused_kernels: true # false
   trust_remote_code: false
   lora_rank: 8
   lora_alpha: 16
   target_modules: all-linear 
   exclude_modules: null
   dtype: float16
   torch_dtype: float16
 actor:
   entropy_from_logits_with_chunking: true
   entropy_checkpointing: false # true
   ppo_mini_batch_size: 64
   ppo_micro_batch_size_per_gpu: 1
   use_dynamic_bsz: False
   ppo_max_token_len_per_gpu: 30000 # 16384 n * ${data.max_prompt_length} + ${data.max_response_length}
   grad_clip: 1.0
   clip_ratio: 0.2
   use_remove_padding: false # true
   use_fused_kernels: false
   policy_loss:
     loss_mode: "vanilla"

   entropy_coeff: 0
   use_kl_loss: true # True for original GRPO
   kl_loss_coef: 0.1  # for grpo
   ppo_epochs: 1
   loss_agg_mode: token-mean
   ulysses_sequence_parallel_size: 1 # sp size
   optim:
     lr: 2e-5
     lr_warmup_steps: 15 # Prioritized. Negative values mean delegating to lr_warmup_steps_ratio.
     lr_warmup_steps_ratio: 0.03  # the total steps will be injected during runtime
     min_lr_ratio: 0.0   # only used with cosine lr scheduler, default to 0.0
     num_cycles: 0.5     # only used with cosine lr scheduler, default to 0.5
     warmup_style: constant  # select from constant/cosine
   strategy: fsdp2
   fsdp_config:
     offload_policy: false # true
     param_offload: false # true
     optimizer_offload: true # true
   checkpoint:
     save_contents: ['model', 'optimizer', 'extra', "hf_model"]
 ref:
   log_prob_micro_batch_size_per_gpu: 2
   use_remove_padding: false # true
   use_fused_kernels: false
 rollout:
   temperature: 1.0
   gpu_memory_utilization: 0.3
   tensor_model_parallel_size: 1
   name: vllm
   mode: sync
   top_p: 1
   top_k: -1
   ignore_eos: False
   enforce_eager: true
   free_cache_engine: true
   multi_stage_wake_up: false
   load_format: safetensors # dummy_dtensor for random weight init
   max_num_batched_tokens: 30000
   max_model_len: 4096 # 8192 # 32768
   prompt_length: 1024 # 16435
   response_length: 3072 # 4096 # 16333
   max_num_seqs: 1024
   log_prob_micro_batch_size_per_gpu: 2
   stop_token_ids: [151643, 151645]
   do_sample: True
   n: 8 # > 1 for grpo, rloo
   use_remove_padding: false # true
   use_fused_kernels: false

custom_reward_function:
  path: /job/verl/verl/utils/reward_score/boxed_verify.py
  name: compute_score

algorithm:
  adv_estimator: grpo # gae
  use_kl_in_reward: False

trainer:
  balance_batch: True
  total_training_steps: 1202
  total_epochs: 1000
  project_name: qwen2.5-1.5_grpo_alignment
  experiment_name: grpo_training
  logger: ['console']
  log_val_generations: 0
  nnodes: 1
  n_gpus_per_node: 1
  save_freq: 153
  test_freq: 153
  val_before_train: False
  critic_warmup: 0
  max_actor_ckpt_to_keep: 10
  resume_mode: disable 
  resume_from_path: null
  remove_previous_ckpt_in_save: False
  del_local_ckpt_after_load: True
  default_local_dir: /train_output
  ray_wait_register_center_timeout: 600
  
cmd: |
  # переименовывание файлов
  mv /job/train_data_* /job/train.parquet 2>/dev/null || true
  mv /job/val_data_* /job/val.parquet 2>/dev/null || true
  mv /job/grpo_config_yaml_* /job/grpo_config.yaml 2>/dev/null || true
 
  mkdir -p /job/verl
  mkdir -p /job/verl_config
  
  cp -r /job/verl/verl /job/verl/ 2>/dev/null || true
  
  cp /job/grpo_config.yaml /job/verl_config/grpo_config.yaml 2>/dev/null || true
  
  VERL_SRC=$(find /job -maxdepth 1 -type d -name "verl_dir_*" -print -quit)
  if [ -n "$VERL_SRC" ]; then
    echo "Найдена исходная папка VERL: $VERL_SRC"
    rm -rf /job/verl/* 2>/dev/null || true
    cp -r "$VERL_SRC"/* /job/verl/ 2>/dev/null || true
    echo "Содержимое $VERL_SRC скопировано в /job/verl/"
  else
    echo "Используем существующую папку /job/verl"
  fi
  
  # проверка правильности структуры
  echo "=== ПРОВЕРКА СТРУКТУРЫ ==="
  find /job -name "main_ppo.py" -type f && echo "main_ppo.py найден - УСПЕХ"
  find /job -name "grpo_config.yaml" -type f && echo "grpo_config.yaml найден - УСПЕХ" 
  find /job -name "train.parquet" -type f && echo "train.parquet найден - УСПЕХ"
  find /job -name "val.parquet" -type f && echo "val.parquet найден - УСПЕХ"
  find /job -name "setup.py" -type f && echo "setup.py найден - УСПЕХ"
  find /job -name "ppo_trainer.yaml" -type f && echo "ppo_trainer.yaml найден - УСПЕХ"
  
  # установка библиотек
  pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121
  pip install math-verify

  # Установка build dependencies для flash-attn
  echo "=== Установка build dependencies ==="
  pip install --upgrade pip setuptools wheel packaging ninja cmake psutil
  
  # Ранняя установка flash-attn (до verl, чтобы проверить совместимость)
  echo "=== РАННЯЯ ПРОВЕРКА: установка flash-attn ==="
  # Сначала пробуем найти готовый wheel
  if ! pip install --no-cache-dir --no-build-isolation \
    --extra-index-url https://wheels.vllm.ai/stable flash-attn 2>&1 | tee /tmp/flash_attn_install.log; then
    echo "Wheel не найден, пробуем установить из исходников (это займет время)..."
    # Для сборки из исходников нужны дополнительные зависимости
    pip install --no-cache-dir packaging ninja
    # Устанавливаем flash-attn с разрешением сборки из исходников
    # --no-build-isolation критично, чтобы использовать уже установленный torch
    if ! pip install --no-cache-dir --no-build-isolation flash-attn 2>&1 | tee -a /tmp/flash_attn_install.log; then
      echo "ERROR: Не удалось установить flash-attn. См. /tmp/flash_attn_install.log"
      echo "Последние строки лога:"
      tail -30 /tmp/flash_attn_install.log
      exit 1
    fi
  fi
  
  # Быстрая проверка импорта flash-attn
  python - <<'PY' || exit 1
  try:
    from flash_attn.bert_padding import pad_input, unpad_input
    from flash_attn.ops.triton.cross_entropy import cross_entropy_loss
    print("✓ flash-attn успешно установлен и импортируется")
  except ImportError as e:
    print(f"✗ ERROR: flash-attn не может быть импортирован: {e}")
    exit(1)
  PY
  
  cd /job/verl
  pip install -e .[vllm]

  python3 -m verl.trainer.main_ppo \
    --config-path=/job/verl_config \
    --config-name=grpo_config 
    
 
inputs:
  - train.parquet: TRAIN_DATA
  - val.parquet: VAL_DATA
  - verl: VERL_DIR
  - grpo_config.yaml: GRPO_CONFIG_YAML

outputs:
  - output_dir: OUTPUT_DIR
  
cloud-instance-type: g1.1

