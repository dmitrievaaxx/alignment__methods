hydra:
  searchpath:
    - /job/verl/verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  train_files: /job/train.parquet
  val_files: /job/val.parquet
  max_prompt_length: 1024
  max_response_length: 3072
  train_batch_size: 256
  filter_overlong_prompts: True
  apply_chat_template_kwargs: {}
  filter_overlong_prompts_workers: 12
  prompt_key: prompt
  reward_fn_key: data_source
  return_raw_chat: True

actor_rollout_ref:
 model:
   path: "Qwen/Qwen2.5-1.5B-Instruct"
   enable_gradient_checkpointing: true
   enable_activation_offload: true # true
   use_remove_padding: true # true
   use_fused_kernels: true # false
 actor:
   entropy_from_logits_with_chunking: true
   entropy_checkpointing: false # true
   ppo_mini_batch_size: 64
   ppo_micro_batch_size_per_gpu: 2
   use_dynamic_bsz: False
   ppo_max_token_len_per_gpu: 30000 # 16384 n * ${data.max_prompt_length} + ${data.max_response_length}
   grad_clip: 1.0
   clip_ratio: 0.2
   policy_loss:
     loss_mode: "vanilla"

   entropy_coeff: 0
   use_kl_loss: False # True for original GRPO
   kl_loss_coef: 0.0  # for grpo
   ppo_epochs: 1
   loss_agg_mode: token-mean
   ulysses_sequence_parallel_size: 1 # sp size
   optim:
     lr: 1e-6
     lr_warmup_steps: 15 # Prioritized. Negative values mean delegating to lr_warmup_steps_ratio.
     lr_warmup_steps_ratio: 0.03  # the total steps will be injected during runtime
     min_lr_ratio: 0.0   # only used with cosine lr scheduler, default to 0.0
     num_cycles: 0.5     # only used with cosine lr scheduler, default to 0.5
     warmup_style: constant  # select from constant/cosine
   strategy: fsdp2
   fsdp_config:
     offload_policy: false # true
     param_offload: false # true
     optimizer_offload: true # true
   checkpoint:
     save_contents: ['model', 'optimizer', 'extra', "hf_model"]
 ref:
   log_prob_micro_batch_size_per_gpu: 2
 rollout:
   temperature: 1.0
   gpu_memory_utilization: 0.6
   tensor_model_parallel_size: 1
   name: vllm
   mode: sync
   top_p: 1
   top_k: -1
   ignore_eos: False
   enforce_eager: true
   free_cache_engine: true
   multi_stage_wake_up: false
   load_format: safetensors # dummy_dtensor for random weight init
   max_num_batched_tokens: 30000
   max_model_len: 4096 # 8192 # 32768
   prompt_length: 1024 # 16435
   response_length: 3072 # 4096 # 16333
   max_num_seqs: 1024
   log_prob_micro_batch_size_per_gpu: 2
   stop_token_ids: [151643, 151645]
   do_sample: True
   n: 8 # > 1 for grpo, rloo

custom_reward_function:
  path: /job/verl/verl/utils/reward_score/boxed_verify.py
  name: compute_score

algorithm:
  adv_estimator: grpo # gae
  use_kl_in_reward: False

trainer:
  balance_batch: True
  total_training_steps: 1202
  total_epochs: 1000
  project_name: qwen2.5-1.5_grpo_alignment
  experiment_name: grpo_training
  logger: ['console', 'wandb']
  log_val_generations: 0
  nnodes: 1
  n_gpus_per_node: 1
  save_freq: 153
  test_freq: 153
  val_before_train: False
  critic_warmup: 0
  max_actor_ckpt_to_keep: 10
  resume_mode: disable 
  resume_from_path: null
  remove_previous_ckpt_in_save: False
  del_local_ckpt_after_load: True
  default_local_dir: /train_output
  
cmd: |
  # переименовывание файлов
  mv /job/train_data_* /job/train.parquet 2>/dev/null || true
  mv /job/val_data_* /job/val.parquet 2>/dev/null || true
  mv /job/config_yaml_* /job/config.yaml 2>/dev/null || true
 
  mkdir -p /job/verl
  mkdir -p /job/verl_config
  
  cp -r /job/verl/verl /job/verl/ 2>/dev/null || true
  
  cp /job/config.yaml /job/verl_config/config.yaml 2>/dev/null || true
  
  VERL_SRC=$(find /job -maxdepth 1 -type d -name "verl_dir_*" -print -quit)
  if [ -n "$VERL_SRC" ]; then
    echo "Найдена исходная папка VERL: $VERL_SRC"
    rm -rf /job/verl/* 2>/dev/null || true
    cp -r "$VERL_SRC"/* /job/verl/ 2>/dev/null || true
    echo "Содержимое $VERL_SRC скопировано в /job/verl/"
  else
    echo "Используем существующую папку /job/verl"
  fi
  
  # проверка правильности структуры
  echo "=== ПРОВЕРКА СТРУКТУРЫ ==="
  find /job -name "main_ppo.py" -type f && echo "main_ppo.py найден - УСПЕХ"
  find /job -name "config.yaml" -type f && echo "config.yaml найден - УСПЕХ" 
  find /job -name "train.parquet" -type f && echo "train.parquet найден - УСПЕХ"
  find /job -name "val.parquet" -type f && echo "val.parquet найден - УСПЕХ"
  find /job -name "setup.py" -type f && echo "setup.py найден - УСПЕХ"
  find /job -name "ppo_trainer.yaml" -type f && echo "ppo_trainer.yaml найден - УСПЕХ"
  
  # установка библиотек
  pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121
  
  cd /job/verl
  pip install -e .[vllm]

  PYTHONPATH=/job/verl:$PYTHONPATH torchrun --standalone --nnodes=1 --nproc_per_node=1 \
    -m verl.trainer.main_ppo \
    --config-path=/job/verl_config \
    --config-name=config 
    
 
inputs:
  - train.parquet: TRAIN_DATA
  - val.parquet: VAL_DATA
  - verl: VERL_DIR
  - config.yaml: CONFIG_YAML

outputs:
  - output_dir: OUTPUT_DIR
  
cloud-instance-type: g1.1
