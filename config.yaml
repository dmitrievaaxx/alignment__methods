name: sft-gsm8k-1epoch-5steps
desc: Run VERL SFT trainer for 1 epoch but stop after 5 steps

cmd: |
  export PYTHONIOENCODING=utf-8
  export LANG=C.UTF-8
  export PIP_ROOT_USER_ACTION=ignore
  export WANDB_MODE=offline
  export WANDB_DIR=/job/output/wandb
  export CLEARML_API_HOST=https://api.clear.ml
  export CLEARML_WEB_HOST=https://app.clear.ml
  export CLEARML_FILES_HOST=https://files.clear.ml
  export CLEARML_API_ACCESS_KEY="1115V427IJEV3GB0UZMCHFD8XPCLO9"
  export CLEARML_API_SECRET_KEY="3k9gBL0lsd9iKBzHvaUpjwXOYtgI7HObE9De98qMDnrBQ0WZQrwITY9Q2PcR4kWWmPs"

  echo "Installing packages..."
  pip install --upgrade pip
  pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121
  pip install packaging ninja wheel setuptools psutil
  pip install flash-attn --no-build-isolation --no-cache-dir --extra-index-url https://wheels.vllm.ai/stable
  pip install omegaconf clearml
  pip install -r "${REQUIREMENTS_FILE}"
  
  mkdir -p /job
  cp -r "${VERL_DIR}" /job/verl
  mkdir -p /job/output/wandb /job/output/checkpoints
  cd /job/verl

  echo "Launching SFT training for 1 epoch (5 steps)..."
  PYTHONPATH=/job:$PYTHONPATH \
    torchrun --standalone --nnodes=1 --nproc_per_node=1 \
    -m verl.trainer.fsdp_sft_trainer \
    data.train_files="${TRAIN_DATA}" \
    data.val_files="${VAL_DATA}" \
    data.train_batch_size=32 \
    data.micro_batch_size_per_gpu=2 \
    data.max_length=1024 \
    data.multiturn.enable=true \
    data.multiturn.messages_key=messages \
    model.partial_pretrain="Qwen/Qwen2.5-1.5B-Instruct" \
    model.strategy=fsdp \
    model.enable_gradient_checkpointing=true \
    model.lora_rank=8 \
    model.lora_alpha=16 \
    model.target_modules='["q_proj","k_proj","v_proj","o_proj"]' \
    optim.lr=2e-5 \
    optim.warmup_steps_ratio=0.03 \
    trainer.project_name="aligment-methods" \
    trainer.experiment_name="sft_5steps_test" \
    trainer.total_epochs=1 \
    trainer.total_training_steps=5 \
    trainer.save_freq=5 \
    trainer.test_freq=5 \
    trainer.logger="['console', 'wandb', 'clearml']" \
    trainer.seed=42 \
    trainer.default_local_dir="/job/output/checkpoints" \
    trainer.default_hdfs_dir=null

  echo "Packaging outputs..."
  mkdir -p "${OUTPUT_DIR}"
  find /job/output/wandb -type l -delete || true
  rm -rf /job/output/wandb/wandb/latest-run || true
  cd /job/output
  tar -czf /job/output/output.tar.gz checkpoints wandb training_completed.txt 2>/dev/null || tar -czf /job/output/output.tar.gz checkpoints wandb 2>/dev/null
  cp /job/output/output.tar.gz "${OUTPUT_DIR}/"
  echo "Archive created: ${OUTPUT_DIR}/output.tar.gz"
  tar -tf "${OUTPUT_DIR}/output.tar.gz" | head -20
  echo "All files prepared for download"

inputs:
  - data/sft_train.parquet: TRAIN_DATA
  - data/sft_val.parquet: VAL_DATA
  - verl/verl: VERL_DIR
  - verl/requirements.txt: REQUIREMENTS_FILE

outputs:
  - output_dir/output.tar.gz: OUTPUT_DIR

cloud-instance-type: g1.1