# configs/sft_config.yaml
defaults:
  - sft_trainer    
  - _self_         

data:
  train_files: ${TRAIN_DATA}
  val_files: ${VAL_DATA}
  train_batch_size: 128
  micro_batch_size_per_gpu: 2
  max_length: 1024
  truncation: error
  multiturn:
    enable: true
    messages_key: messages

model:
  partial_pretrain: "Qwen/Qwen2.5-1.5B-Instruct"
  enable_gradient_checkpointing: true
  lora_rank: 8
  lora_alpha: 16
  target_modules: ["q_proj","k_proj","v_proj","o_proj"]
  fsdp_config:
    model_dtype: bfloat16
    cpu_offload: false
    offload_params: false
  external_lib: null
  use_liger: false
  strategy: fsdp

optim:
  lr: 3e-6
  lr_warmup_steps_ratio: 0.03
  num_cycles: 1.0
  lr_scheduler: cosine

ulysses_sequence_parallel_size: 1
use_remove_padding: true

trainer:
  default_local_dir: "/job/output/checkpoints"
  default_hdfs_dir: null
  project_name: "aligment-methods"
  experiment_name: "sft_5steps_test"
  group: "sft-test-group"
  job_type: "sft"
  total_epochs: 5
  total_training_steps: null
  logger: ['console', 'wandb', 'clearml']
  seed: 42
  save_freq: 58
  test_freq: 7
  nnodes: 1
  n_gpus_per_node: 1
  max_ckpt_to_keep: 30
  resume_mode: disable
  resume_from_path: null
  device: cuda
  checkpoint:
    save_contents: ['model', 'optimizer', 'extra', 'hf_model']
    load_contents: ['model', 'optimizer', 'extra', 'hf_model']
